# Donn√©es d'Entra√Ænement - M√âDIA SCAN

Ce dossier contient les donn√©es d'entra√Ænement collect√©es pour le mod√®le de classification d'articles.

## üìã Structure des Donn√©es

Chaque article collect√© contient les champs suivants :

```json
{
  "text": "Contenu complet de l'article...",
  "label": "Politique",
  "title": "Titre de l'article",
  "author": "Nom de l'auteur",
  "date": "2025-11-16 00:00:00",
  "url": "https://...",
  "category_raw": "politique",
  "media": "AIB Media",
  "image_url": "https://...",
  "tags": ["tag1", "tag2"],
  "scraped_at": "2025-11-16T10:00:00"
}
```

## üè∑Ô∏è Labels Standardis√©s

Les donn√©es sont √©tiquet√©es avec les labels suivants :

- **Politique** : Articles sur le gouvernement, √©lections, partis politiques
- **√âconomie** : Budget, commerce, entreprises, finance
- **S√©curit√©** : Arm√©e, police, terrorisme, d√©fense
- **Sant√©** : H√¥pitaux, maladies, √©pid√©mies, soins
- **Culture** : Arts, festivals, musique, cin√©ma, patrimoine
- **Sport** : Football, comp√©titions, √©quipes sportives
- **International** : Diplomatie, coop√©ration, actualit√©s mondiales
- **Soci√©t√©** : √âducation, jeunesse, famille, environnement
- **Autre** : Articles ne correspondant √† aucune cat√©gorie ci-dessus

## üìä Fichiers G√©n√©r√©s

- `aib_training_data.json` : Donn√©es collect√©es depuis AIB Media
- `lefaso_training_data.json` : Donn√©es collect√©es depuis Lefaso.net
- `merged_training_data.json` : Fusion de toutes les sources (g√©n√©r√© avec --merge)

## üöÄ Collecte de Donn√©es

### Collecter depuis tous les m√©dias

```bash
python collect_training_data.py --scrapers aib lefaso --max-pages 10
```

### Collecter depuis un m√©dia sp√©cifique

```bash
python collect_training_data.py --scrapers aib --max-pages 5
```

### Analyser les donn√©es existantes

```bash
python collect_training_data.py --analyze-only
```

### Fusionner toutes les donn√©es

```bash
python collect_training_data.py --analyze-only --merge
```

## üìà Bonnes Pratiques

### 1. √âquilibre des Classes

Assurez-vous d'avoir un nombre √©quilibr√© d'articles par label pour √©viter les biais :

```bash
python collect_training_data.py --analyze-only
```

Cette commande affiche la distribution des labels et le ratio de d√©s√©quilibre.

### 2. Diversit√© des Sources

Collectez depuis plusieurs m√©dias pour avoir une meilleure g√©n√©ralisation :

- AIB Media (moderne, WordPress)
- Lefaso.net (traditionnel, SPIP)
- Autres m√©dias burkinab√®

### 3. Volume de Donn√©es

Recommandations pour l'entra√Ænement :

- **Minimum** : 100 articles par label (800 total)
- **Recommand√©** : 500+ articles par label (4000+ total)
- **Optimal** : 1000+ articles par label (8000+ total)

### 4. Qualit√© des Donn√©es

Avant l'entra√Ænement, v√©rifiez :

- ‚úÖ Pas d'articles vides (champ `text` non vide)
- ‚úÖ Labels corrects et normalis√©s
- ‚úÖ Pas de doublons (m√™me URL)
- ‚úÖ Texte de qualit√© (pas de HTML, pas de navigation)

## üîß Configuration Avanc√©e

### Ajouter un nouveau scraper de training

1. Cr√©er `scrapers/[media]_training_scraper.py`
2. H√©riter de `BaseMediaScraper`
3. Utiliser `normalize_label()` pour normaliser les cat√©gories
4. Sauvegarder dans `train_data/[media]_training_data.json`

Exemple de structure :

```python
from config.label_mapping import normalize_label

class MonMediaTrainingScraper(BaseMediaScraper):
    custom_settings = {
        "FEEDS": {
            str(TRAIN_DATA_DIR / "monmedia_training_data.json"): {
                "format": "json",
                "encoding": "utf8",
            },
        },
    }

    def parse_article(self, response):
        # ...
        label = normalize_label(category)

        yield {
            'text': content,
            'label': label,
            # ... autres champs
        }
```

### Normalisation des Labels

Le fichier `config/label_mapping.py` contient le mapping des cat√©gories brutes vers les labels standardis√©s.

Pour ajouter de nouvelles correspondances :

```python
CATEGORY_TO_LABEL = {
    "nouvelle_categorie": "Politique",
    # ...
}
```

## üìù Exemple d'Utilisation Compl√®te

```bash
# 1. Collecter les donn√©es
python collect_training_data.py --scrapers aib lefaso --max-pages 20

# 2. Analyser la distribution
python collect_training_data.py --analyze-only

# 3. Si besoin, ajuster et recollecterr certaines cat√©gories

# 4. Fusionner toutes les donn√©es
python collect_training_data.py --analyze-only --merge

# 5. Utiliser merged_training_data.json pour l'entra√Ænement
```

## ‚ö†Ô∏è Notes Importantes

- Les scrapers respectent le `robots.txt` et incluent des d√©lais entre requ√™tes
- Les donn√©es sont sauvegard√©es en mode "append" (pas d'√©crasement)
- Pour recommencer, supprimez les fichiers JSON existants
- V√©rifiez toujours la qualit√© des donn√©es avant l'entra√Ænement

## üéØ Prochaines √âtapes

1. Collecter suffisamment de donn√©es (min. 100 par label)
2. Nettoyer et valider les donn√©es
3. Diviser en ensembles train/validation/test
4. Entra√Æner le mod√®le de classification
5. √âvaluer les performances sur l'ensemble de test